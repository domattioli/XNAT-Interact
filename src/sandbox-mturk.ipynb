{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pydicom.dataset import Dataset, FileDataset, FileMetaDataset\n",
    "from pydicom.uid import generate_uid, ExplicitVRLittleEndian\n",
    "from pydicom.sequence import Sequence\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pydicom\n",
    "from dateutil import parser\n",
    "from dateutil.tz import gettz\n",
    "\n",
    "import base64\n",
    "import cv2\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = r'C:\\Users\\dmattioli\\OneDrive - University of Iowa\\Downloads\\Batch_5197022_batch_results.csv'\n",
    "\n",
    "meta_cols_to_keep = ['HITId', 'HITTypeId', 'Reward', 'AssignmentId', 'WorkerId', 'AcceptTime', 'SubmitTime', 'WorkTimeInSeconds', 'Approve', 'Reject']\n",
    "img_col_name = r'Answer.femurSemSeg.labeledImage.pngImageData'\n",
    "meta_cols_to_keep.append(img_col_name)\n",
    "\n",
    "# Define a dictionary to map timezone abbreviations to timezones\n",
    "tzinfos = {\n",
    "    'PST': gettz('America/Los_Angeles'),\n",
    "    'PDT': gettz('America/Los_Angeles')\n",
    "}\n",
    "\n",
    "def import_mturk_csv(ffn: str) -> dict:\n",
    "    df = pd.read_csv(ffn)\n",
    "\n",
    "    # Drop any columns that are not in the meta_cols_to_keep list.\n",
    "    df.drop(columns=[col for col in df.columns if col not in meta_cols_to_keep], inplace=True)\n",
    "\n",
    "    # Name the table with the batch_id.\n",
    "    df.name = re.search(r'Batch_(\\d+)_batch_results.csv', ffn).group(1)\n",
    "\n",
    "    # Convert the SubmitTime and AcceptTime columns to datetime using dateutil.parser with tzinfos\n",
    "    df['SubmitTime'] = df['SubmitTime'].apply(lambda x: parser.parse(x, tzinfos=tzinfos))\n",
    "    df['AcceptTime'] = df['AcceptTime'].apply(lambda x: parser.parse(x, tzinfos=tzinfos))\n",
    "\n",
    "    # Create sub dataframes according to the grouped HITId values.\n",
    "    grouped = df.groupby('HITId')\n",
    "    sub_dfs = {hit_id: group.reset_index(drop=True) for hit_id, group in grouped}\n",
    "    return sub_dfs\n",
    "\n",
    "\n",
    "def decode_image(base64_str: str) -> np.ndarray:\n",
    "    img_bytes = base64.b64decode(base64_str)\n",
    "    img_array = np.frombuffer(img_bytes, dtype=np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.uint32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m     fn \u001b[38;5;241m=\u001b[39m Path(pn) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     65\u001b[0m     file_names\u001b[38;5;241m.\u001b[39mappend(fn)\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mdcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Pick a random filename to dicomread and display the image stored.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m fn \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(file_names)\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\dataset.py:2129\u001b[0m, in \u001b[0;36mDataset.save_as\u001b[1;34m(self, filename, write_like_original)\u001b[0m\n\u001b[0;32m   2114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_as\u001b[39m(\n\u001b[0;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2116\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mos.PathLike[AnyStr]\u001b[39m\u001b[38;5;124m\"\u001b[39m, BinaryIO],\n\u001b[0;32m   2117\u001b[0m     write_like_original: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2119\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write the :class:`Dataset` to `filename`.\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m \n\u001b[0;32m   2121\u001b[0m \u001b[38;5;124;03m    Wrapper for pydicom.filewriter.dcmwrite, passing this dataset to it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;124;03m        Write a DICOM file from a :class:`FileDataset` instance.\u001b[39;00m\n\u001b[0;32m   2128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2129\u001b[0m     \u001b[43mpydicom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdcmwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_like_original\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:1151\u001b[0m, in \u001b[0;36mdcmwrite\u001b[1;34m(filename, dataset, write_like_original)\u001b[0m\n\u001b[0;32m   1149\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(deflated)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1151\u001b[0m         \u001b[43m_write_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_like_original\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m caller_owns_file:\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:887\u001b[0m, in \u001b[0;36m_write_dataset\u001b[1;34m(fp, dataset, write_like_original)\u001b[0m\n\u001b[0;32m    884\u001b[0m fp\u001b[38;5;241m.\u001b[39mis_little_endian \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mbool\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mis_little_endian)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;66;03m# Write non-Command Set elements now\u001b[39;00m\n\u001b[1;32m--> 887\u001b[0m \u001b[43mwrite_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0x00010000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:643\u001b[0m, in \u001b[0;36mwrite_dataset\u001b[1;34m(fp, dataset, parent_encoding)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_little_endian\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_implicit_VR\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe set appropriately before saving\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m     )\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mis_original_encoding:\n\u001b[1;32m--> 643\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_ambiguous_vr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_little_endian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    645\u001b[0m dataset_encoding \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m    646\u001b[0m     Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m    647\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecificCharacterSet\u001b[39m\u001b[38;5;124m'\u001b[39m, parent_encoding)\n\u001b[0;32m    648\u001b[0m )\n\u001b[0;32m    650\u001b[0m fpStart \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:248\u001b[0m, in \u001b[0;36mcorrect_ambiguous_vr\u001b[1;34m(ds, is_little_endian)\u001b[0m\n\u001b[0;32m    246\u001b[0m             correct_ambiguous_vr(item, is_little_endian)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mVR \u001b[38;5;129;01min\u001b[39;00m AMBIGUOUS_VR:\n\u001b[1;32m--> 248\u001b[0m         \u001b[43mcorrect_ambiguous_vr_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_little_endian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:204\u001b[0m, in \u001b[0;36mcorrect_ambiguous_vr_element\u001b[1;34m(elem, ds, is_little_endian)\u001b[0m\n\u001b[0;32m    201\u001b[0m     ds\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setitem__\u001b[39m(elem\u001b[38;5;241m.\u001b[39mtag, elem)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[43m_correct_ambiguous_vr_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_little_endian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to resolve ambiguous VR for tag \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melem\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m    208\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\filewriter.py:125\u001b[0m, in \u001b[0;36m_correct_ambiguous_vr_element\u001b[1;34m(elem, ds, is_little_endian)\u001b[0m\n\u001b[0;32m    121\u001b[0m     elem_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m         elem\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mVM \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cast(Sequence[Any], elem\u001b[38;5;241m.\u001b[39mvalue)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem_value, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m--> 125\u001b[0m         elem\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_numbers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_little_endian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyte_type\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# 'OB or OW' and dependent on WaveformBitsAllocated\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtag \u001b[38;5;129;01min\u001b[39;00m _ob_ow_tags:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# If WaveformBitsAllocated is > 8 then OW, otherwise may be\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m#   OB or OW.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m#   See PS3.3 C.10.9.1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dmattioli\\Projects\\XNAT-Interact\\.venv\\lib\\site-packages\\pydicom\\values.py:386\u001b[0m, in \u001b[0;36mconvert_numbers\u001b[1;34m(byte_string, is_little_endian, struct_format)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# \"=\" means use 'standard' size, needed on 64-bit systems.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m bytes_per_value \u001b[38;5;241m=\u001b[39m calcsize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m struct_format)\n\u001b[1;32m--> 386\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m%\u001b[39m bytes_per_value \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BytesLengthException(\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected total bytes to be an even multiple of bytes per value. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbytes_per_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.uint32' has no len()"
     ]
    }
   ],
   "source": [
    "def create_dicom( subject_uid: str, user_uid: str, HIT_Id: str, data: pd.DataFrame ) -> Dataset: # bws: list, uids: dict ) -> Dataset:\n",
    "        # Create a dicom file to represent the segmentations for the referenced dicom file\n",
    "        file_meta = FileMetaDataset()\n",
    "        # file_meta.MediaStorageSOPClassUID = self._mediastoragesopclassuid \n",
    "        # file_meta.MediaStorageSOPInstanceUID = pydicomUID.generate_uid()\n",
    "        # file_meta.ImplementationClassUID = pydicomUID.generate_uid()\n",
    "        # file_meta.TransferSyntaxUID = self._transfersyntaxuid\n",
    "        file_meta.TransferSyntaxUID = ExplicitVRLittleEndian\n",
    "        \n",
    "        dcm = Dataset()\n",
    "        dcm.StudyInstanceUID = subject_uid # to-do: should be pydicom compliant already, will need to come back to this with a revised metatable\n",
    "        # HIT_id = str( uids['HITIDS'][0] )\n",
    "        # dcm.UID = pydicomUID( HIT_id ) # HIT Id -- to-do: will need to redo this table to include pydicom compliant uids\n",
    "        dcm.file_meta = file_meta\n",
    "        # dcm.ReferencedSOPInstanceUID = original_dcm.SOPInstanceUID\n",
    "        # dcm.InstanceCreationDate, dcm.InstanceCreationTime = self._datetime.date, self._datetime.time\n",
    "        dcm.InstanceCreatorUID = user_uid\n",
    "        dcm.ImageType = ['DERIVED', 'PRIMARY', 'SEGMENTATION']\n",
    "        dcm.Modality = 'SEG' # Key Object Selection\n",
    "        dcm.LargestImagePixelValue, dcm.SmallestImagePixelValue = np.uint( 1 ), np.uint( 0 )\n",
    "        dcm.add_new( 0x00080050, 'LO', HIT_Id )  # Use LO VR for Accession Number\n",
    "        dcm.SeriesDescription = 'Instructions Refinement - Iteration N' # TBD**********\n",
    "        dcm.SegmentSequence = create_segment_sequence( data )\n",
    "        dcm.is_little_endian = True\n",
    "        dcm.is_implicit_VR = False\n",
    "        return dcm\n",
    "\n",
    "\n",
    "def create_segment_sequence( sub_df: pd.DataFrame ) -> Sequence: # uids: dict, bws: list ) -> Sequence:\n",
    "    segment_sequence = Sequence()\n",
    "    for idx, row in sub_df.iterrows():\n",
    "        seg = Dataset()\n",
    "        seg.SegmentNumber = idx # Worker ID here\n",
    "        seg.OperatorsName = str( row['WorkerId'] ) # Worker ID here\n",
    "        seg.add_new( (0x0019, 0x10a0), 'LO', \"Operators' Name : WorkerID\" )\n",
    "        seg.SegmentLabel = str( row['AssignmentId'] ) # To:do: Assignment ID here to denote the unique submission of the worker for this HIT\n",
    "        # seg.SegmentAlgorithmType, seg.SegmentAlgorithmName, seg.SegmentationType = _SegmentAlgorithmType, self._SegmentAlgorithmName, self._SegmentationType\n",
    "        \n",
    "        # Create a CodeSequence for the SegmentedPropertyCategoryCodeSequence attribute\n",
    "        code_sequence = Dataset()\n",
    "        # code_sequence.CodeValue, code_sequence.CodingSchemeDesignator, code_sequence.CodeMeaning = self._object_type_code, self._CodingSchemeDesignator, self._object_type\n",
    "        seg.SegmentedPropertyCategoryCodeSequence = Sequence( [code_sequence] )\n",
    "        \n",
    "        # Create a CodeSequence for the SegmentedPropertyTypeCodeSequence attribute -- same as above\n",
    "        seg.SegmentedPropertyTypeCodeSequence = Sequence( [code_sequence.copy()] )\n",
    "        \n",
    "        # seg.SegmentDescription = 'Instructions Refinement - Iteration N'\n",
    "        bw = decode_image( row[img_col_name] )\n",
    "        seg.PixelData, seg.Rows, seg.Columns = bw.tobytes(), bw.shape[0], bw.shape[1]\n",
    "        segment_sequence.append( seg )\n",
    "    #     self._metatables.add_new_item( table_name='MTURK_WORKER_IDS', item_name=seg.SegmentNumber, extra_columns_values={'MTURK_HIT_ID': HIT_id } )\n",
    "    #     self._metatables.add_new_item( table_name='MTURK_ASSIGNMENT_IDS', item_name=seg.SegmentLabel, extra_columns_values={'MTURK_HIT_ID': HIT_id, 'MTURK_WORKER_ID': seg.SegmentNumber } )\n",
    "    # self._metatables.add_new_item( table_name='MTURK_HIT_IDS', item_name=HIT_id )\n",
    "    return segment_sequence\n",
    "\n",
    "\n",
    "\n",
    "# For each file in dicom_datasets, write as a dicom file to pn, storing the file names\n",
    "file_names=[]\n",
    "pn = r'C:\\Users\\dmattioli\\OneDrive - University of Iowa\\Downloads\\tmp\\example_mturk_compiled_data'\n",
    "tst = import_mturk_csv(ffn)\n",
    "for key, value in tst.items():\n",
    "    dcm = create_dicom( subject_uid=generate_uid(), user_uid=generate_uid(), HIT_Id=key, data=value )\n",
    "    fn = Path(pn) / f'{key}'\n",
    "    file_names.append(fn)\n",
    "    dcm.save_as(fn)\n",
    "\n",
    "# Pick a random filename to dicomread and display the image stored.\n",
    "fn = random.choice(file_names)\n",
    "ds = pydicom.dcmread(fn)\n",
    "img = ds.pixel_array\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicom_from_mturk(HITs: dict) -> list:\n",
    "    dicom_datasets = []\n",
    "\n",
    "    for hit_id, df in HITs.items():\n",
    "        # Create a new DICOM dataset object\n",
    "        dicom_uid = generate_uid()\n",
    "        file_meta = Dataset()\n",
    "        file_meta.MediaStorageSOPClassUID = '1.2.840.10008.5.1.4.1.1.88.11'  # Comprehensive SR Storage\n",
    "        file_meta.MediaStorageSOPInstanceUID =dicom_uid\n",
    "        file_meta.ImplementationClassUID = generate_uid()\n",
    "        file_meta.TransferSyntaxUID = ExplicitVRLittleEndian\n",
    "\n",
    "        ds = FileDataset(fn, {}, file_meta=file_meta, preamble=b\"\\0\" * 128)\n",
    "        ds.SOPClassUID = '1.2.840.10008.5.1.4.1.1.88.11'  # Comprehensive SR Storage\n",
    "        ds.SOPInstanceUID = dicom_uid\n",
    "        # ds.StudyInstanceUID = dicom_uid # need to derive this from the existing xnat server -- should be the uid we used for the uploaded source images.\n",
    "        # ds.SeriesInstanceUID = dicom_uid # same.\n",
    "        ds.Modality = 'SR'\n",
    "        # ds.SeriesNumber = 1\n",
    "        # ds.InstanceNumber = 1\n",
    "        ds.PatientID = 'REDACTED'\n",
    "        ds.PatientName = 'REDACTED'\n",
    "        ds.PatientBirthDate = '19000101'\n",
    "        ds.PatientSex = 'O'\n",
    "        date_time = df['SubmitTime'].iloc[0]\n",
    "        ds.ContentDate = date_time.strftime('%Y%m%d')\n",
    "        ds.ContentTime = date_time.strftime('%H%M%S')\n",
    "        # ds.AccessionNumber = hit_id # hit_id is too long and is throwing an error (length 30 but max is 16 for VR SH)\n",
    "        ds.add_new(0x00080050, 'LO', hit_id)  # Use LO VR for Accession Number\n",
    "        # print( len(hit_id))\n",
    "\n",
    "\n",
    "        # Add the metadata from the DataFrame to the DICOM dataset\n",
    "        image_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a content item for each row\n",
    "            content_item = Dataset()\n",
    "            content_item.ValueType = 'TEXT'\n",
    "            content_item.add_new(0x0040A160, 'UT', f\"WorkerId: {row['WorkerId']}, Reward: {row['Reward']}, AssignmentId: {row['AssignmentId']}\")\n",
    "            content_item.ConceptNameCodeSequence = Sequence([Dataset()])\n",
    "            content_item.ConceptNameCodeSequence[0].CodeValue = '121071'\n",
    "            content_item.ConceptNameCodeSequence[0].CodingSchemeDesignator = 'DCM'\n",
    "            content_item.ConceptNameCodeSequence[0].CodeMeaning = 'Textual description'\n",
    "            ds.add_new(0x0040A730, 'SQ', Sequence([content_item]))\n",
    "\n",
    "            # extract the encoded image string.\n",
    "            image_data.append( decode_image( row[img_col_name] ) )\n",
    "        \n",
    "        # Combine all images into an NxRxC array; store the combined image in the DICOM dataset.\n",
    "        combined_image = np.stack(image_data, axis=0)\n",
    "        ds.Rows, ds.Columns, ds.SamplesPerPixel = combined_image.shape[1], combined_image.shape[2], combined_image.shape[3]\n",
    "        ds.PixelRepresentation = 0  # 0 for unsigned integers\n",
    "        ds.BitsStored = 8\n",
    "        ds.HighBit = 7\n",
    "        ds.PhotometricInterpretation, ds.BitsAllocated = 'RGB', 8\n",
    "        ds.PixelData = combined_image.tobytes()\n",
    "        ds.is_little_endian = True\n",
    "        ds.is_implicit_VR = False\n",
    "\n",
    "        # print(f\"{'---'*50}\\n{ds}\\n{'---'*50}\\n\")\n",
    "        dicom_datasets.append(ds)\n",
    "\n",
    "    return dicom_datasets\n",
    "\n",
    "poo = import_mturk_csv(ffn)\n",
    "dicom_datasets = create_dicom_from_mturk(poo)\n",
    "\n",
    "pn = r'C:\\Users\\dmattioli\\OneDrive - University of Iowa\\Downloads\\tmp\\example_mturk_compiled_data'\n",
    "# For each file in dicom_datasets, write as a dicom file to pn, storing the file names\n",
    "file_names=[]\n",
    "for i, ds in enumerate(dicom_datasets):\n",
    "    # Filename will be the dicom's accessionNumber for now\n",
    "    fn = Path(pn) / f'{ds.AccessionNumber}'\n",
    "    file_names.append(fn)\n",
    "    ds.save_as(fn)\n",
    "\n",
    "# Pick a random filename to dicomread and display the image stored.\n",
    "fn = random.choice(file_names)\n",
    "ds = pydicom.dcmread(fn)\n",
    "img = ds.pixel_array\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OtherDicomSession():\n",
    "    def __init__( self, batch_ffn: str, login: XNATLogin, xnat_connection: XNATConnection, metatables: MetaTables, object: str, print_out: bool=False ):\n",
    "        self._batch_ffn = batch_ffn\n",
    "        self._login = login\n",
    "        self._xnat_connection = xnat_connection\n",
    "        self._metatables = metatables\n",
    "        assert self._metatables.item_exists( table_name='Acquisition_sites', item_name='AMAZON_MECHANICAL_TURK' )\n",
    "        assert self._metatables.table_exists( 'Segmentation_Objects' )\n",
    "        assert self._metatables.item_exists( table_name='Segmentation_Objects', item_name=object )\n",
    "        self._acquisition_site, self._object = 'AMAZON_MECHANICAL_TURK', object\n",
    "\n",
    "        \n",
    "        # metatables.add_new_item( table_name='MTURK_BATCH_IDS', item_name='Segmentation', extra_columns_values={'task':'Segmentation'} )\n",
    "\n",
    "\n",
    "        # attributes that metatables will eventually need to store -- to-do:\n",
    "        self._object, self._object_type = object, 'BONE'\n",
    "        self._object_code, self._object_type_code, self._CodingSchemeDesignator = 'T-12760', 'T-32000', 'SRT' # Code for 'Femur', 'Bone', according to copilot gpt\n",
    "        self._SegmentAlgorithmType, self._SegmentAlgorithmName, self._SegmentationType = 'MANUAL', 'MTURK', 'BINARY'\n",
    "        \n",
    "        self._mediastoragesopclassuid, self._transfersyntaxuid = pydicomUID('1.2.840.10008.5.1.1.4'), pydicomUID('1.2.840.10008.1.2') # Basic Grayscale Image Box SOP Class, Implicit VR Little Endian\n",
    "        self._print_out = print_out    \n",
    "\n",
    "        # Create a new df with the number of rows equal to the number of groups. create a column for the respective Input.image_url\n",
    "        self._raw_data = pd.read_csv( self._batch_ffn )\n",
    "        self._datetime = USCentralDateTime( self._raw_data.loc[0, 'ApprovalTime'] )\n",
    "        icol = self._get_pngImageData_col()\n",
    "        self._raw_data['BinaryImages'] = [self.convert_base64_to_np_array( row[icol] ) for i, row in self._raw_data.iterrows()]\n",
    "        self._grouped_data = self._raw_data.groupby( 'Input.image_url' )\n",
    "        self._df = pd.DataFrame( index=range( len( self._grouped_data ) ), columns=['SUBJECT_XNAT_UID',\n",
    "                                                                                    'image_hashes_mt_uid', 'subject_mt_uid',\n",
    "                                                                                    'FN', 'IS_VALID', 'IMAGE_HASHES', 'BWS' 'DCMDATA', 'UID_INFO', 'FFN'] )\n",
    "        \n",
    "        # for each row of self._df compute the ImageHash for the s3url in that row's Image\n",
    "        self._df['FFN'] = [group for group, data in self._grouped_data]\n",
    "        self._df['FN'] = [s3url.split('/')[-1] for s3url in self._df['FFN']]\n",
    "        self._df['IMAGE_HASHES'] = [self._read_image( s3url ) for s3url in self._df['FFN']]\n",
    "        self._df['UID_INFO'] = self._grouped_data.apply(lambda g: {'HITIDS': g['HITId'].tolist(), \n",
    "                                                            'ASSIGNMENTIDS': g['AssignmentId'].tolist(), \n",
    "                                                            'WORKERIDS': g['WorkerId'].tolist()}).reset_index(drop=True)\n",
    "        \n",
    "        # Store a list of the binary images in the BWS col\n",
    "        self._df['BWS'] = [data['BinaryImages'].to_list() for group, data in self._grouped_data]\n",
    "\n",
    "        # given that hash_str is a property of each item in the objects stored in each row of self._df['ImageHashes'], apply self._metatables.item_exists( table_name='IMAGE_HASHES', item_name=hash_str) via list comprehension\n",
    "        self._df['IS_VALID'] = [self._metatables.item_exists(table_name='IMAGE_HASHES', item_name=item.hash_str) for item in self._df['IMAGE_HASHES']]\n",
    "\n",
    "        # Create a dicom file for each row such that the semantic segmentation data is stacked\n",
    "        self._create_DCMDATA()\n",
    "\n",
    "        # create a list of dataframes such that each dataframe groups together the self._df rows that correspond to images belonging to the same subject.\n",
    "        self._group_rows_by_subject()\n",
    "        \n",
    "        \n",
    "    def _get_table( self, table_name: str ) -> pd.DataFrame: #to-do: move this to metatables class definition.\n",
    "        assert self._metatables.table_exists( table_name ), f'Table {table_name} does not exist in metatables'\n",
    "        return self._metatables.tables[table_name]\n",
    "\n",
    "    # def _get_item( self, table_name: str, item_name: str ) -> pd.Series: #to-do: move this to metatables class definition\n",
    "    #     # table name represents the table in self._metatables.tables that we want. item_name is the row of that table that we want. return the row as a pd.series\n",
    "    #     return self._get_table(table_name).loc[item_name]\n",
    "\n",
    "\n",
    "    def _read_image( self, s3url: str ) -> ImageHash:\n",
    "        response = requests.get( s3url, stream=True )\n",
    "        response.raw.decode_content = True\n",
    "        arr = np.asarray( bytearray( response.raw.read() ), dtype=np.uint8 )\n",
    "        return ImageHash( metatables=self._metatables, img=cv2.imdecode( arr, cv2.IMREAD_GRAYSCALE ) ) # img = Image.open( response.raw )\n",
    "\n",
    "    # write a method that returns the column name containing the substring .pngImageData\n",
    "    def _get_pngImageData_col( self ) -> str:\n",
    "        return [c for c in self._raw_data.columns if '.pngImageData' in c][0]\n",
    "\n",
    "    def convert_base64_to_np_array( self, b64_str: str ) -> np.ndarray:\n",
    "        return cv2.imdecode( np.frombuffer( base64.b64decode( b64_str ), np.uint8 ), cv2.IMREAD_GRAYSCALE )\n",
    "\n",
    "    def _derive_subject_instance_num( self, subject_xnat_uid: str, hash_str: str, image_hashes_mt_uid: str ) -> str: # filename that we need to mimic\n",
    "            \n",
    "        pass\n",
    "\n",
    "    def _create_DCMDATA( self ):\n",
    "        hash_table, subject_table, registered_user_table = self._get_table( 'IMAGE_HASHES' ), self._get_table( 'SUBJECTS' ), self._get_table( 'REGISTERED_USERS' )\n",
    "        user_mt_uid = registered_user_table.loc[registered_user_table['NAME'] == self._login.validated_username.upper(), 'UID'].values[0]\n",
    "        for i, row in self._df.iterrows():\n",
    "            if not row['IS_VALID']:\n",
    "                continue\n",
    "            image_hashes_mt_uid = row['IMAGE_HASHES'].hash_str.upper()\n",
    "            subject_mt_uid = hash_table.loc[hash_table['NAME'] == image_hashes_mt_uid, 'SUBJECT'].values[0] # type: ignore\n",
    "            self._df.loc[i,'SUBJECT_XNAT_UID'] = subject_table.loc[subject_table['UID'] == subject_mt_uid, 'NAME'].values[0] # type: ignore\n",
    "            self._df.loc[i,'image_hashes_mt_uid'] = image_hashes_mt_uid\n",
    "            self._df.loc[i,'subject_mt_uid'] = subject_mt_uid\n",
    "            dcm = self._create_dicom( subject_name=self._df.loc[i,'SUBJECT_XNAT_UID'], user_uid=user_mt_uid, bws=row['BWS'], uids=row['UID_INFO'] )\n",
    "            self._df.loc[i, 'DCMDATA'] = [dcm]\n",
    "\n",
    "    def _create_dicom( self, subject_name: str, user_uid: str, bws: list, uids: dict ) -> pydicomDataset:\n",
    "        # Create a dicom file to represent the segmentations for the referenced dicom file\n",
    "        file_meta = pydicomFileMetaDataset()\n",
    "        file_meta.MediaStorageSOPClassUID = self._mediastoragesopclassuid \n",
    "        # file_meta.MediaStorageSOPInstanceUID = pydicomUID.generate_uid()\n",
    "        # file_meta.ImplementationClassUID = pydicomUID.generate_uid()\n",
    "        file_meta.TransferSyntaxUID = self._transfersyntaxuid\n",
    "        \n",
    "        dcm = pydicom.Dataset()\n",
    "        dcm.StudyInstanceUID = pydicomUID( subject_name ) # to-do: should be pydicom compliant already, will need to come back to this with a revised metatable\n",
    "        HIT_id = str( uids['HITIDS'][0] )\n",
    "        # dcm.UID = pydicomUID( HIT_id ) # HIT Id -- to-do: will need to redo this table to include pydicom compliant uids\n",
    "        dcm.file_meta = file_meta\n",
    "        # dcm.ReferencedSOPInstanceUID = original_dcm.SOPInstanceUID\n",
    "        dcm.InstanceCreationDate, dcm.InstanceCreationTime = self._datetime.date, self._datetime.time\n",
    "        dcm.InstanceCreatorUID = generate_pydicomUID() # to-do: will need to redo this table to include pydicom compliant uids\n",
    "        # dcm.InstanceCreatorUID = user_uid # to-do: will need to redo this table to include pydicom compliant uids\n",
    "        dcm.ImageType = ['DERIVED', 'PRIMARY', 'SEGMENTATION']\n",
    "        dcm.Modality = 'KO' # Key Object Selection\n",
    "        dcm.LargestImagePixelValue, dcm.SmallestImagePixelValue = np.uint( 1 ), np.uint( 0 )\n",
    "        dcm.SeriesDescription = 'Instructions Refinement - Iteration N'\n",
    "        dcm.SegmentSequence = self._create_segment_sequence( uids, bws )\n",
    "        return dcm\n",
    "\n",
    "    def _create_segment_sequence( self, uids: dict, bws: list ) -> Sequence:\n",
    "        segment_sequence = Sequence()\n",
    "        for idx, bw in enumerate( bws ):\n",
    "            seg = Dataset()\n",
    "            seg.SegmentNumber = idx # Worker ID here\n",
    "            seg.OperatorsName = str( uids['WORKERIDS'][idx] ) # Worker ID here\n",
    "            seg.add_new( (0x0019, 0x10a0), 'LO', \"Operators' Name : WorkerID\" )\n",
    "            seg.SegmentLabel = str( uids['ASSIGNMENTIDS'][idx] ) # To:do: Assignment ID here to denote the unique submission of the worker for this HIT\n",
    "            seg.SegmentAlgorithmType, seg.SegmentAlgorithmName, seg.SegmentationType = self._SegmentAlgorithmType, self._SegmentAlgorithmName, self._SegmentationType\n",
    "            \n",
    "            # Create a CodeSequence for the SegmentedPropertyCategoryCodeSequence attribute\n",
    "            code_sequence = Dataset()\n",
    "            code_sequence.CodeValue, code_sequence.CodingSchemeDesignator, code_sequence.CodeMeaning = self._object_type_code, self._CodingSchemeDesignator, self._object_type\n",
    "            seg.SegmentedPropertyCategoryCodeSequence = Sequence( [code_sequence] )\n",
    "            \n",
    "            # Create a CodeSequence for the SegmentedPropertyTypeCodeSequence attribute -- same as above\n",
    "            seg.SegmentedPropertyTypeCodeSequence = Sequence( [code_sequence.copy()] )\n",
    "            \n",
    "            # seg.SegmentDescription = 'Instructions Refinement - Iteration N'\n",
    "            seg.PixelData, seg.Rows, seg.Columns = bw.tobytes(), bw.shape[0], bw.shape[1]\n",
    "            segment_sequence.append( seg )\n",
    "        #     self._metatables.add_new_item( table_name='MTURK_WORKER_IDS', item_name=seg.SegmentNumber, extra_columns_values={'MTURK_HIT_ID': HIT_id } )\n",
    "        #     self._metatables.add_new_item( table_name='MTURK_ASSIGNMENT_IDS', item_name=seg.SegmentLabel, extra_columns_values={'MTURK_HIT_ID': HIT_id, 'MTURK_WORKER_ID': seg.SegmentNumber } )\n",
    "        # self._metatables.add_new_item( table_name='MTURK_HIT_IDS', item_name=HIT_id )\n",
    "        return segment_sequence\n",
    "    \n",
    "    def _group_rows_by_subject( self ):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def write( self, idcm: int, zip_dest: Opt[str] = None, print_out: Opt[bool] = False ) -> str: #write individiual dicom files to a zipped folder\n",
    "        \n",
    "        if not self._df.loc[idcm, 'IS_VALID']:\n",
    "            # if print_out:\n",
    "            print( f'***Session is invalid; could be for several reasons. try evaluating whether all of the image hash_strings already exist in the matatable.' )\n",
    "        pass\n",
    "\n",
    "    def catalog_new_data( self ):\n",
    "        self._metatables.save( print_out=print_out )\n",
    "        pass\n",
    "\n",
    "batch_ffn = r'C:\\Users\\dmattioli\\Projects\\XNAT\\data\\examples\\MTurkSemanticSegmentation_Example_2.csv'\n",
    "poo = OtherDicomSession( batch_ffn, validated_login, connection, metatables, object='Humerus' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTurkSemanticSegmentation( ScanFile ):\n",
    "    ''' # Example usage:\n",
    "    print( MTurkSemanticSegmentation( pd.read_csv( r'...\\\\data\\\\examples\\\\MTurkSemanticSegmentation_Example_File.csv' ) ) )\n",
    "    '''\n",
    "    def __init__( self, assignment: pd.Series, metatables: MetaTables ): #to-do: allow for different input types eg batch file data or pulled-from-xnat data\n",
    "        super().__init__( metatables )  # Call the __init__ method of the base class\n",
    "        self._validate_input( assignment )\n",
    "        self._read_image()\n",
    "        self._extract_target_object_info() #to-do\n",
    "        self._extract_date_and_time()\n",
    "        # self._extract_uid_info()\n",
    "        self._extract_pngImageData()\n",
    "        self._check_data_validity()\n",
    "\n",
    "    @property\n",
    "    def bw( self ) -> np.ndarray:   return self._bw\n",
    "    \n",
    "    def _validate_input( self, assignment: pd.Series ):\n",
    "        assert len( set(self.mturk_batch_col_names) - set(assignment.columns) ) == 0, f\"Missing required columns: {set(self.mturk_batch_col_names) - set(assignment.columns)}\"\n",
    "        self._metadata = assignment.loc[0]\n",
    "        img_s3_url = assignment.loc[0,'Input.image_url']\n",
    "        assert self.is_s3_url( img_s3_url ), f'Input.image_url column of inputted data series (row) must be an s3 url: {img_s3_url}'\n",
    "        self._ffn = self.metadata['Input.image_url']\n",
    "        self._bw, self._acquisition_site = self.image.dummy_image(), 'AMAZON_MECHANICAL_TURK' #to-do: this is copy-pasted from the MetaTables, need to figure out how to query it.\n",
    "\n",
    "    def _check_data_validity( self ): # need to check that the pnd image *does* exist in metatables\n",
    "        self._is_valid = self.image.in_img_hash_metatable and not self.is_similar_to_template_image()\n",
    "    \n",
    "    def _extract_target_object_info( self ):\n",
    "        pass\n",
    "\n",
    "    def _extract_date_and_time( self ):\n",
    "        self._datetime = USCentralDateTime( self.metadata.loc['SubmitTime'] )\n",
    "\n",
    "    def _extract_pngImageData( self ):\n",
    "        pngImageData_index = [i for i, c in enumerate( self.metadata.index.to_list() ) if '.pngImageData' in c]\n",
    "        self._bw = self.convert_base64_to_np_array( self.metadata.iloc[pngImageData_index[0]] )\n",
    "\n",
    "     \n",
    "    def __str__( self ):\n",
    "        return f'{self.__class__.__name__}:\\t{self.ffn}\\nIs Valid:\\t{self.is_valid}\\nAcquisition Site: {self.acquisition_site}\\nGroup:\\t\\t{self.group}\\nDatetime:\\t{self.datetime}\\nUID Info: {self.uid_info}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
